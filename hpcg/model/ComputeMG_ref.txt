operations
	1x fill vector with 0's
	1x access struct field
	1x compare accumulator, 0
	2x load int
	#if A.mgData == 0 (if next coarse level is not defined)
		1x call ComputeSYMGS_ref
	#else
		6x access struct field
		2x store int to counter
		3x compare accumulator, 0
		1x call ComputeRestriction_ref
		1x call ComputeProlongation_ref
		1x recurse to next coarse level
		loop numberOfPresmootherSteps+numberOfPostsmootherSteps times:
			1x compare counter, int
			1x incr counter
			1x call ComputeSYMGS_ref
			1x add
			1x load int from accumulator
			1x store int to accumulator
	#end if
	
	
bytes loaded (excluding counters and accumulators, assuming all pointers are the same size, A_n is the nth coarsifid version of the matrix)
	
	2*sizeof(int)
	+ bytesLoaded(ComputeRestriction_ref(A_0))
	+ bytesLoaded(ComputeProlongation_ref(A_0))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_0))
	
	+ 2*sizeof(int)
	+ bytesLoaded(ComputeRestriction_ref(A_1))
	+ bytesLoaded(ComputeProlongation_ref(A_1))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_1))
	
	+ 2*sizeof(int)
	+ bytesLoaded(ComputeRestriction_ref(A_2))
	+ bytesLoaded(ComputeProlongation_ref(A_2))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_2))
	
	+ 2*sizeof(int)
	+ bytesLoaded(ComputeSYMGS_ref(A_3))
	
	
	= 8*sizeof(int) + 24*sizeof(void*) + 7*sizeof(local_int_t)
	
	+ n_0*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ n_1*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ n_2*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	
	+ n_3*(6*sizeof(void*) + 2*sizeof(int) + 6*sizeof(<datatype>))
	+ nnz_3*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)
		*(9*sizeof(void*) + 3*sizeof(local_int_t)
		+ n_0*(6*sizeof(void*) + 2*sizeof(int) + 6*sizeof(<datatype>))
		+ nnz_0*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
		+ n_1*(6*sizeof(void*) + 2*sizeof(int) + 6*sizeof(<datatype>))
		+ nnz_1*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
		+ n_2*(6*sizeof(void*) + 2*sizeof(int) + 6*sizeof(<datatype>))
		+ nnz_2*(2*sizeof(local_int_t) + 4*sizeof(<datatype>)))
	
	
	calculated for actual setup (sizeof(void*) -> 8 bytes, sizeof(local_int_t) -> sizeof(int) -> 4 bytes)
			
		420
		+ n_0*(120 + 16*sizeof(<datatype>))
		+ n_1*(120 + 16*sizeof(<datatype>))
		+ n_2*(120 + 16*sizeof(<datatype>))
		+ n_3*(56 + 6*sizeof(<datatype>))
		+ nnz_0*(16 + 8*sizeof(<datatype>))
		+ nnz_1*(16 + 8*sizeof(<datatype>))
		+ nnz_2*(16 + 8*sizeof(<datatype>))
		+ nnz_3*(8 + 4*sizeof(<datatype>))
			
		with 32-bit floats
			420
			+ n_0*(184)
			+ n_1*(184)
			+ n_2*(184)
			+ n_3*(80)
			+ nnz_0*(48)
			+ nnz_1*(48)
			+ nnz_2*(48)
			+ nnz_3*(24)
		
		with 64-bit floats
			420
			+ n_0*(248)
			+ n_1*(248)
			+ n_2*(248)
			+ n_3*(104)
			+ nnz_0*(80)
			+ nnz_1*(80)
			+ nnz_2*(80)
			+ nnz_3*(40)
				
		theoretical  - ~38.51% decrease
