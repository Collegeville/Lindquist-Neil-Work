operations
	1x fill vector with 0's
	1x access struct field
	1x compare accumulator, 0
	2x load int
	#if A.mgData == 0 (if next coarse level is not defined)
		1x call ComputeSYMGS_ref
	#else
		6x access struct field
		2x store int to counter
		3x compare accumulator, 0
		1x call ComputeRestriction_ref
		1x call ComputeProlongation_ref
		1x recurse to next coarse level
		loop numberOfPresmootherSteps+numberOfPostsmootherSteps times:
			1x compare counter, int
			1x incr counter
			1x call ComputeSYMGS_ref
			1x add
			1x load int from accumulator
			1x store int to accumulator
	#end if
	
	
bytes loaded (excluding counters and accumulators, assuming all pointers are the same size, A_n is the nth coarsifid version of the matrix)
	
	8
	+ bytesLoaded(ComputeSYMGS_ref(A_0)
		*(1 + numberOfPresmootherSteps+numberOfPostsmootherSteps)
	+ bytesLoaded(ComputeSYMGS_ref(A_1)
	+ bytesLoaded(ComputeSYMGS_ref(A_2)
	+ bytesLoaded(ComputeSYMGS_ref(A_3)
	
	+ bytesLoaded(ComputeRestriction_ref(A_0))
	+ bytesLoaded(ComputeRestriction_ref(A_1))
	+ bytesLoaded(ComputeRestriction_ref(A_2))
	+ bytesLoaded(ComputeRestriction_ref(A_3))
	
	+ bytesLoaded(ComputeProlongation_ref(A_0))
	+ bytesLoaded(ComputeProlongation_ref(A_1))
	+ bytesLoaded(ComputeProlongation_ref(A_2))
	+ bytesLoaded(ComputeProlongation_ref(A_3))
		
	= 8
	+ (3*sizeof(void*) + sizeof(local_int_t)
		+ n_0*(4*sizeof(void*) + 2*sizeof(int) + 4*sizeof(<datatype>))
		+ nnz_0*(2*sizeof(local_int_t) + 4*sizeof(<datatype>)))
	  *(1 + numberOfPresmootherSteps+numberOfPostsmootherSteps)
	+ 3*sizeof(void*) + sizeof(local_int_t)
		+ n_1*(4*sizeof(void*) + 2*sizeof(int) + 4*sizeof(<datatype>))
		+ nnz_1*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ 3*sizeof(void*) + sizeof(local_int_t)
		+ n_2*(4*sizeof(void*) + 2*sizeof(int) + 4*sizeof(<datatype>))
		+ nnz_2*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ 3*sizeof(void*) + sizeof(local_int_t)
		+ n_3*(4*sizeof(void*) + 2*sizeof(int) + 4*sizeof(<datatype>))
		+ nnz_3*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
		
	+ 4*sizeof(void*) + sizeof(local_int_t) + n_0*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 4*sizeof(void*) + sizeof(local_int_t) + n_1*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 4*sizeof(void*) + sizeof(local_int_t) + n_2*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 4*sizeof(void*) + sizeof(local_int_t) + n_3*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	
	+ 3*sizeof(void*) + sizeof(local_int_t) + n_0*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 3*sizeof(void*) + sizeof(local_int_t) + n_1*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 3*sizeof(void*) + sizeof(local_int_t) + n_2*(sizeof(local_int_t) + 2*sizeof(<datatype>))
	+ 3*sizeof(void*) + sizeof(local_int_t) + n_3*(sizeof(local_int_t) + 2*sizeof(<datatype>))
		
		
		
	= 8
	+ (3*sizeof(void*) + sizeof(local_int_t)
		+ n_0*(4*sizeof(void*) + 2*sizeof(int) + 4*sizeof(<datatype>))
		+ nnz_0*(2*sizeof(local_int_t) + 4*sizeof(<datatype>)))
	  *(numberOfPresmootherSteps+numberOfPostsmootherSteps)
	 	
	+ 40*sizeof(void*) + 12*sizeof(local_int_t)
	
	+ n_0*(4*sizeof(void*) + sizeof(local_int_t) + 2*sizeof(int) + 6*sizeof(<datatype>))
	+ n_1*(4*sizeof(void*) + sizeof(local_int_t) + 2*sizeof(int) + 6*sizeof(<datatype>))
	+ n_2*(4*sizeof(void*) + sizeof(local_int_t) + 2*sizeof(int) + 6*sizeof(<datatype>))
	+ n_3*(4*sizeof(void*) + sizeof(local_int_t) + 2*sizeof(int) + 6*sizeof(<datatype>))
				
	+ nnz_0*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ nnz_1*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ nnz_2*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	+ nnz_3*(2*sizeof(local_int_t) + 4*sizeof(<datatype>))
	
	
	
	calculated for actual setup (sizeof(void*) -> 8 bytes, sizeof(local_int_t) -> sizeof(int) -> 4 bytes)
			
		432
		+ n_0*(124 + 14*sizeof(<datatype>))
		+ n_1*(44 + 6*sizeof(<datatype>))
		+ n_2*(44 + 6*sizeof(<datatype>))
		+ n_3*(44 + 6*sizeof(<datatype>))	
		+ nnz_0*(24 + 12*sizeof(<datatype>))
		+ nnz_1*(8 + 4*sizeof(<datatype>))
		+ nnz_2*(8 + 4*sizeof(<datatype>))
		+ nnz_3*(8 + 4*sizeof(<datatype>))
			
		with 32-bit floats
			432
			+ n_0*(180)
			+ n_1*(68)
			+ n_2*(68)
			+ n_3*(68)	
			+ nnz_0*(72)
			+ nnz_1*(24)
			+ nnz_2*(24)
			+ nnz_3*(24)
		
		with 64-bit floats
			432
			+ n_0*(236)
			+ n_1*(92)
			+ n_2*(92)
			+ n_3*(92)	
			+ nnz_0*(120)
			+ nnz_1*(40)
			+ nnz_2*(40)
			+ nnz_3*(40)
				
		theoretical  - ~38.88% decrease
