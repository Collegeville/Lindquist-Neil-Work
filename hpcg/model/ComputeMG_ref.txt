operations
	1x fill vector with 0's
	1x access struct field
	1x compare accumulator, 0
	2x load int
	#if A.mgData == 0 (if next coarse level is not defined)
		1x call ComputeSYMGS_ref
	#else
		6x access struct field
		2x store int to counter
		3x compare accumulator, 0
		1x call ComputeSPMV_ref
		1x call ComputeRestriction_ref
		1x call ComputeProlongation_ref
		1x recurse to next coarse level
		loop numberOfPresmootherSteps+numberOfPostsmootherSteps times:
			1x compare counter, int
			1x incr counter
			1x call ComputeSYMGS_ref
			1x add
			1x load int from accumulator
			1x store int to accumulator
	#end if


bytes loaded (excluding counters and accumulators, assuming all pointers are the same size, A_n is the nth coarsifid version of the matrix)

	2*sizeof(int)
	+ bytesLoaded(ComputeSPMV_ref(A_0))
	+ bytesLoaded(ComputeRestriction_ref(A_0))
	+ bytesLoaded(ComputeProlongation_ref(A_0))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_0))

	+ 2*sizeof(int)
	+ bytesLoaded(ComputeSPMV_ref(A_1))
	+ bytesLoaded(ComputeRestriction_ref(A_1))
	+ bytesLoaded(ComputeProlongation_ref(A_1))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_1))

	+ 2*sizeof(int)
	+ bytesLoaded(ComputeSPMV_ref(A_2))
	+ bytesLoaded(ComputeRestriction_ref(A_2))
	+ bytesLoaded(ComputeProlongation_ref(A_2))
	+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)*bytesLoaded(ComputeSYMGS_ref(A_2))

	+ 2*sizeof(int)
	+ bytesLoaded(ComputeSYMGS_ref(A_3))

	= {
		well cached:
			= 8*sizeof(int) + 30*sizeof(void*) + 10*sizeof(local_int_t)

			+ n_0*(2*sizeof(void*) + sizeof(int) + 2*sizeof(local_int_t) + 4*sizeof(<datatype>))
			+ n_1*(2*sizeof(void*) + sizeof(int) + 2*sizeof(local_int_t) + 4*sizeof(<datatype>))
			+ n_2*(2*sizeof(void*) + sizeof(int) + 2*sizeof(local_int_t) + 4*sizeof(<datatype>))

			+ nnz_0*(sizeof(local_int_t)+2*sizeof(<datatype>))
			+ nnz_1*(sizeof(local_int_t)+2*sizeof(<datatype>))
			+ nnz_2*(sizeof(local_int_t)+2*sizeof(<datatype>))

			+ 4 + n_3*(56 + 6*sizeof(<datatype>)) + nnz_3*(8 + 2*sizeof(<datatype>))

			+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)
				*(12 + n_0*(56 + 6*sizeof(<datatype>)) + nnz_0*(8 + 2*sizeof(<datatype>))
					+ n_1*(56 + 6*sizeof(<datatype>)) + nnz_1*(8 + 2*sizeof(<datatype>))
					+ n_2*(56 + 6*sizeof(<datatype>)) + nnz_2*(8 + 2*sizeof(<datatype>)))
		poorly cached:
			nnz_3*2*sizeof(<datatype>)
			+ (numberOfPresmootherSteps+numberOfPostsmootherSteps)
				*(nnz_0*2*sizeof(<datatype>)
				  + nnz_1*2*sizeof(<datatype>)
				  + nnz_2*2*sizeof(<datatype>))
	}


	calculated for actual setup (sizeof(void*) -> 8 bytes, sizeof(local_int_t) -> sizeof(int) -> 4 bytes, numberOfPresmootherSteps = numberOfPostsmootherSteps = 1)
		well cached:
			  340

			+ n_0*(140 + 16*sizeof(<datatype>))
			+ n_1*(140 + 16*sizeof(<datatype>))
			+ n_2*(140 + 16*sizeof(<datatype>))
			+ n_3*(56 + 6*sizeof(<datatype>))

			+ nnz_0*(20 + 6*sizeof(<datatype>))
			+ nnz_1*(20 + 6*sizeof(<datatype>))
			+ nnz_2*(20 + 6*sizeof(<datatype>))
			+ nnz_3*(8 + 2*sizeof(<datatype>))

		poorly cached:
			  nnz_0*4*sizeof(<datatype>)
			+ nnz_1*4*sizeof(<datatype>)
			+ nnz_2*4*sizeof(<datatype>)
			+ nnz_3*2*sizeof(<datatype>)

		with 32-bit floats

			{
				well cached:
					340 + 204*(n_0+n_1+n_2) + 80*n_3
						+ 44*(nnz_0+nnz_1+nnz_2) + 16*nnz_3
				poorly cached:
					16*(nnz_0 + nnz_1 + nnz_2) + 8*nnz_3
			}
			≈{
				well cached:
					58.8568*nnz
				poorly cached:
					18.21384*nnz
			}

		with 64-bit floats

			{
				well cached:
					340 + 268*(n_0 + n_1 + n_2) + 104*n_3
						+ 68*(nnz_0 + nnz_1 + nnz_2) + 24*nnz_3
				poorly cached:
					32*(nnz_0+nnz_1+nnz_2) + 16*nnz_3
			}
			≈{
				well cached:
					88.92479*nnz
				poorly cached:
					36.42768*nnz
			}

		theoretical  -  33.81% decrease in well cached bytes
						50.00% decrease in poorly cached bytes
