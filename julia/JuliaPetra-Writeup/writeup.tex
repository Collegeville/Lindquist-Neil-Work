\documentclass[acmsmall]{acmart}
\bibliographystyle{ACM-Reference-Format}

\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{positioning} 

\title{Implementation of Petra in Julia}
\author{Neil Lindquist}

\newcommand{\juliaSnippet}[1]{\texttt{\detokenize{#1}}}

\let\oldtextregister\textregistered
\renewcommand{\textregistered}{\textsuperscript{\tiny\oldtextregister}}

\begin{document}

\maketitle

\section{Introduction}

%TODO go into more detail?
The Petra Object Model is the design used in Trilinos for objects commonly used in solver algorithms.
\cite{OverviewOfTrilinos}
Petra libraries provide parallel, distributed matrices, vectors and graphs for other packages to use.
The Petra object model has previously been implemented in a very stable subset of C++,
in a newer, more advanced subset of C++ and in Java.
%REVIEW should this list of previous implemenetations be a thing?

%TODO does the design need to be its own section?

JuliaPetra is an implementation of the Petra object model in Julia.
This would provide Petra's distributed linear algebra objects for use in Julia solvers.
The design of JuliaPetra follows closely with that of the C++ implementations of the
Petra object model, Epetra and Tpetra.
In particular, the communication APIs, such as \juliaSnippet{Comm} and \juliaSnippet{BlockMap}
are based directly off Epetra and the data structures, such as \juliaSnippet{MultiVector} and
\juliaSnippet{RowMatrix} are based directly off Tpetra.
Additionally, like Epetra and Tpetra, JuliaPetra uses MPI and Single-Program-Multiple-Data as it's parallel model.

\begin{figure}
	%name, type, location
	\newcommand{\typeNode} [3]{\node[#2] (#1) [#3] {\juliaSnippet{#1}};}
	\newcommand{\explicitInheritance}[2]{\draw[->] (#1.south) -- (#2.north);}
	\newcommand{\implicitInheritance}[2]{\draw[dashed,->] (#1.south) -- (#2.north);}
	\begin{tikzpicture}[
		ImplicitInterface/.style={rectangle, draw=black, dashed, minimum size=5mm},
		ExplicitInterface/.style={rectangle, draw=black, minimum size=5mm},
		ConcreteType/.style={rectangle, draw=black, very thick, minimum size=5mm}
		]
		
		\typeNode{Comm}{ExplicitInterface}{}
		
		\typeNode{MPIComm}{ConcreteType}{below=of Comm}
		\explicitInheritance{Comm}{MPIComm}
		
		\typeNode{SerialComm}{ConcreteType}{right=.25 of MPIComm}
		\explicitInheritance{Comm}{SerialComm}
		
		\typeNode{LocalComm}{ConcreteType}{left=.25cm of MPIComm}
		\explicitInheritance{Comm}{LocalComm}
		
		\typeNode{Distributor}{ExplicitInterface}{left=3.25cm of Comm}
		
		\typeNode{SerialDistributor}{ConcreteType}{below=of Distributor}
		\explicitInheritance{Distributor}{SerialDistributor}
		
		\typeNode{MPIDistributor}{ConcreteType}{left=.25cm of SerialDistributor}
		\explicitInheritance{Distributor}{MPIDistributor}
		
		
		
		\typeNode{Directory}{ExplicitInterface}{below=of MPIDistributor}
		
		\typeNode{BasicDirectory}{ConcreteType}{below=of Directory}
		\explicitInheritance{Directory}{BasicDirectory}
		
		\typeNode{BlockMap}{ConcreteType}{right=of Directory}
		
		\typeNode{Import}{ConcreteType}{right=of BlockMap}
		
		\typeNode{Export}{ConcreteType}{right=of Import}
		
		
		
		\typeNode{SrcDistObject}{ImplicitInterface}{below=of Export}
		
		\typeNode{DistObject}{ImplicitInterface}{below=.75cm of SrcDistObject}
		\implicitInheritance{SrcDistObject}{DistObject}
		
		\typeNode{Operator}{ImplicitInterface}{left=of DistObject}
		
		\typeNode{AbstractArray}{ExplicitInterface}{left=of Operator}
		
		\typeNode{MultiVector}{ConcreteType}{below=of AbstractArray}
		\implicitInheritance{DistObject}{MultiVector}
		\explicitInheritance{AbstractArray}{MultiVector}
		
		\typeNode{RowMatrix}{ExplicitInterface}{below=of Operator}
		\implicitInheritance{DistObject}{RowMatrix}
		\implicitInheritance{Operator}{RowMatrix}
		\explicitInheritance{AbstractArray}{RowMatrix}
		
		\typeNode{RowGraph}{ExplicitInterface}{below=of DistObject}
		\implicitInheritance{DistObject}{RowGraph}
		
		\typeNode{CSRMatrix}{ConcreteType}{below=.75cm of RowMatrix}
		\explicitInheritance{RowMatrix}{CSRMatrix}
		
		\typeNode{CSRGraph}{ConcreteType}{below=.75cm of RowGraph}
		\explicitInheritance{RowGraph}{CSRGraph}
		
		
		
		\node[ExplicitInterface] (ExplicitKey) [below=of CSRMatrix] {Abstract Type};
		\node[ImplicitInterface] (ImplicitKey) [left=of ExplicitKey] {Implicit Type};
		\node[ConcreteType]      (ConcreteKey) [right=of ExplicitKey] {Concrete Type};

	\end{tikzpicture}
	\caption{Type hierarchy.}
	\label{fig:type-hierarchy}
\end{figure}

The bottom level of abstraction handles the details of how the problem is distributed.
The \juliaSnippet{Comm} and \juliaSnippet{Distributor} types provides an interface to support
different communication methods.
All multiprocess communication is done through \juliaSnippet{Comm} objects, so new communication
systems can be implemented without affected the objects built on top of it.
The \juliaSnippet{BlockMap}, \juliaSnippet{Directory}, \juliaSnippet{Import} and \juliaSnippet{Export}
types handle the next layer of abstraction.
\juliaSnippet{BlockMap} and \juliaSnippet{Directory} manage where the various indices of the problem are located.
\juliaSnippet{Import} and \juliaSnippet{Export} handle the logic behind redistributing the indices of
the problem around the set of processes.
The \juliaSnippet{SrcDistObject} and \juliaSnippet{DistObject} interfaces provide the necessary
methods for handling the actual redistribution of the data in the various data structures.

The linear algebra objects are built on top of the abstractions provided by the communication
layer.
JuliaPetra has three main linear algebra types.
The first type is the concrete type \juliaSnippet{MultiVector} which holds one or more dense vectors.
The second type is the abstract interface \juliaSnippet{RowMatrix} representing matrices accessed by rows.
The third type is the abstract interface \juliaSnippet{RowGraph} represented graphs accessed by rows.
Row graphs are used to represent the sparsity pattern of row matrices.
Both \juliaSnippet{RowGraph} and \juliaSnippet{RowMatrix} have concrete implementations based on
compressed sparse row, \juliaSnippet{CSRGraph} and \juliaSnippet{CSRMatrix} respectively.
Julia's \juliaSnippet{AbstractArray} is subtyped by both the \juliaSnippet{MultiVector} and \juliaSnippet{RowMatrix} types.
This allows existing Julia code to interact with data in JuliaPetra objects.
The \juliaSnippet{Operator} interface provides an interface for types that provide an
\(y = \alpha A x + \beta y\) operation, such as matrices.
\juliaSnippet{RowMatrix} is the only implementation of \juliaSnippet{Operator} in JuliaPetra.

The type hierarchy is limited by the fact that Julia doesn't support multiple inheritance.%TODO cite?
Interacting with existing code as a 2 dimensional array requires being a subtype of \juliaSnippet{AbstractArray}.
This results in other interfaces not being explicit types, but simply contracts promised in the documentation.
These interfaces include \juliaSnippet{Operator}, \juliaSnippet{SrcDistObject} and \juliaSnippet{DistObject}.
So, any methods that use one of those types accepts any object and assumes the methods required for the interface are present.
Figure~\ref{fig:type-hierarchy} contains the full type hierarchy.

Since JuliaPetra follows the design of Epetra and Tpetra, the library provides the ability to do similar
computations as the existing Petra implementations.
For example, JuliaPetra supports \juliaSnippet{MultiVector} operations such as dot product and norms.
Additionally, sparse matrix - vector products can be computed.

\section{Comparisons with Other Distributed Libraries}

\subsection{Comparing with Epetra}

The APIs for JuliaPetra are fairly similar to those for Epetra,
as Epetra was used as the template for implementing JuliaPetra.
There are some differences between the two.
For example, JuliaPetra takes advantage of higher level structures, such as thrown exceptions and
the \juliaSnippet{AbstractArray} support, where able.
The similarities between JuliaPetra and Epetra can be seen in how similar the respective implementations
of the power method are.

JuliaPetra was able to achieve the same performance as Epetra on large instances of the power method,
as can be seen in the Results section.
Since Julia uses just in time compiling exclusively, it has extra start up costs compared to
the Epetra version. However, since each specialized method needs to be compiled only once,
this is a fixed cost.
Additionally, JuliaPetra uses runtime dispatch in a few locations, especially related to
\juliaSnippet{Comm} objects.  This will also add overhead compared to the Epetra version.

\subsection{Comparing with DistributedArrays.jl}

DistributedArrays.jl is a similar library designed to support distributed arrays in parallel environments.
\cite{DAGithub}
DistributedArrays.jl is designed for Julia's built in parallel model, so each worker processes has a
local chunk of the problem. \cite{JuliaFreshApproach}
When an operation is called on the master process's object, it distributes the task to the respective
processes, then combines the local results into the final result.
DistributedArrays.jl offers a number of advantages over JuliaPetra, however, it is not able to
match the speed of JuliaPetra.

One area where DistributedArrays.jl has an advantage over Julia is that it follows standard
practice for ideomatic Julia code more than JuliaPetra.
For example, it is built around the default Julia parallelization APIs and model.
Another advantage is that DistributedArrays.jl uses an instance of \juliaSnippet{AbstractArray}
for a process's local storage, this allows automatic support for different array structures.
However, DistributedArrays.jl will not work if a single-program-multiple-data parallel model is required.

However, DistributedArrays.jl was not able to match the speed of JuliaPetra.
It was able to get within twice the time of JuliaPetra for the largest problems ran,
but was never able to get within 1.5 times the execution time of JuliaPetra.

\section{Results}

\begin{figure}
\begin{tabular}{|c c|r|r|r||r|r|}
	\hline
		\multicolumn{2}{|c|}{Equations Per Process}
		& JuliaPetra
		& Epetra
		& \multicolumn{1}{m{1.8cm}||}{Distributed\-Arrays.jl}
		& \multicolumn{1}{m{1.75cm}|}{JuliaPetra / Epetra}
		& \multicolumn{1}{m{1.8cm}|}{JuliaPetra / Distributed\-Arrays.jl} \\
	\hline
		\multicolumn{7}{|l|}{4 Processors}\\
	\hline
		100,000			&Average & 0.27690 & 0.19294 & 1.86626 & 1.43515 & 0.14837 \\
		equations		&Minimum & 0.26948 & 0.18933 & 1.59244 & 1.42335 & 0.16922 \\
						&Maximum & 0.28782 & 0.19614 & 2.15375 & 1.46745 & 0.13364 \\
	\hline
		1,000,000		&Average & 3.07809 & 2.38844 & 14.7880 & 1.28875 & 0.20815 \\
		equations		&Minimum & 2.98883 & 2.35483 & 13.2876 & 1.26923 & 0.22493 \\
						&Maximum & 3.15854 & 2.44491 & 18.4698 & 1.29188 & 0.17101 \\
	\hline
		10,000,000		&Average & 33.0215 & 24.6116 & 108.760 & 1.34171 & 0.30362 \\
		equations		&Minimum & 31.5646 & 24.4134 & 107.118 & 1.29292 & 0.29467 \\
						&Maximum & 35.1871 & 24.7888 & 109.561 & 1.41947 & 0.32116 \\
	\hline
		\multicolumn{7}{|l|}{16 Processors}\\
	\hline
		100,000			&Average & 0.58371 & 0.51138 & 3.23072 & 1.15766 & 0.18068 \\
		equations		&Minimum & 0.54744 & 0.49328 & 2.99947 & 1.10979 & 0.18251 \\
						&Maximum & 0.60776 & 0.51907 & 3.47056 & 1.17086 & 0.17512 \\
	\hline
		1,000,000		&Average & 7.46367 & 7.39699 & 21.5343 & 1.00901 & 0.34659 \\
		equations		&Minimum & 7.36098 & 7.32456 & 20.0884 & 1.00497 & 0.36643 \\
						&Maximum & 7.59598 & 7.48964 & 22.8655 & 1.01420 & 0.33220 \\
	\hline
		10,000,000		&Average & 74.6001 & 73.6360 & 151.184 & 1.01310 & 0.49344 \\
		equations		&Minimum & 73.7350 & 73.4646 & 150.411 & 1.00368 & 0.49022 \\
						&Maximum & 74.9087 & 73.8342 & 152.033 & 1.01455 & 0.49271 \\
	\hline
			\multicolumn{7}{|l|}{20 Processors}\\
	\hline
		100,000			&Average & 0.73843 & 0.70837 & 3.14911 & 1.04244 & 0.23449 \\
		equations		&Minimum & 0.73621 & 0.69428 & 2.90237 & 1.06039 & 0.25366 \\
						&Maximum & 0.73967 & 0.71449 & 3.64133 & 1.03524 & 0.20313 \\
	\hline
		1,000,000		&Average & 9.19411 & 9.18584 & 23.2980 & 1.00090 & 0.39463 \\
		equations		&Minimum & 9.14960 & 9.16630 & 22.7740 & 0.99814 & 0.40177 \\
						&Maximum & 9.22002 & 9.21510 & 24.6296 & 1.00053 & 0.37435 \\
	\hline
		10,000,000		&Average & 92.0613 & 91.6512 & 174.402 & 1.00447 & 0.52786 \\
		equations		&Minimum & 91.5029 & 91.2491 & 173.480 & 1.00278 & 0.52746 \\
						&Maximum & 92.5630 & 91.7905 & 175.096 & 1.00842 & 0.52864 \\
	\hline
\end{tabular}

\caption{Timing results in seconds.}
\label{fig:timing-results}
\end{figure}

The timing results can be seen in Figure~\ref{fig:timing-results}.
The timings were obtained by running each power method implementations
with various sized problems and varying numbers of processors.
Each combination of implementation, problem size and number of processors was run five times.
The Julia implementations was run once before collecting the timings, to ensure all code was
compiled before the timing started.
Tests were run with Julia v0.6.0 and Trilinos 12.10.1.
Tests were run on a Red Hat Enterprise Linux Workstation, version 7.3,
with 20 Intel\textregistered Xeon\textregistered CPUs.
%TODO anything else that should be described?


\section{Conclusion}

JuliaPetra is an implementation of the Petra object model in the Julia programming language.
This provides a set of basic, distributed linear algebra objects for building solvers in Julia.
JuliaPetra is able to perform at the same speeds as Epetra and so, allows implementation of
solver code in Julia with the distributed linear algebra support of the Petra object model.

%This shows that...?

\bibliography{bibliography}
\end{document}