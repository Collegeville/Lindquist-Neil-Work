\documentclass[acmsmall]{acmart}
\bibliographystyle{ACM-Reference-Format}

\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{positioning} 

\title{JuliaPetra: An Implementation of the Petra Object Model in Julia}

\author{Neil Lindquist}
\email{neillindquist5@gmail.com}

\acmJournal{TOMS}


\newcommand{\juliaSnippet}[1]{\texttt{\detokenize{#1}}}

\begin{document}

\begin{abstract}
JuliaPetra provides linear algebra data structures that are commonly used in large scale linear solver algorithms.
The data structures in JuliaPetra focus on vectors and sparse matrices, in both serial and
distributed, parallel environments.
The library is written in Julia, a high level programming language with comparable performance to C or Fortran.
JuliaPetra performs as fast as Epetra, an equivalent C++ library, and faster than DistributedArrays.jl, a Julia
library for distributed computations.
\end{abstract}

\maketitle

%TODO improve use of transitions between sentences
%TODO improve inter-paragraph and inter-sectional transitioning

\section{Introduction}

JuliaPetra is an implementation of the Petra object model in Julia.
The Petra object model is the design used in Trilinos for objects commonly used in linear solver algorithms.
\cite{OverviewOfTrilinos}
Petra libraries provide parallel, distributed matrices, vectors and graphs for other packages to use.
By providing a set of interfaces for basic linear algebra structures, Petra libraries provide a way
to develop packages for these types of distributed algorithms that are able to interact and build on each other.
The Petra object model has previously been implemented in both C++ and Java.
%REVIEW should this list of previous implemenetations be in the paper?

%REVIEW does there need to be a section on Julia?
	%There isn't really an equivalent in papers for Fortrain/C programs
	%Julia is less estabilished for hpc though
Julia is a programing language that uses just in time compiling and powerful type inferencing
to obtain the speed of a statically compiled programing language with the productivity of an
interpreted programing language. \cite{JuliaFreshApproach}
By providing speed comparable to that of Fortran or C, Julia is a candidate for doing large scale,
high performance computations.
The Celeste project is an example of large scale computations in Julia,
using 8,192 cores of the Cori Supercomputer
at Lawrence Berkeley National Laboratory. \cite{JuliaFreshApproach}

Like other implementations of the Petra object model,
JuliaPetra provides the ability to do basic linear algebra computations.
These computations include dot products, vector norms and sparse matrix - vector multiplications.
%REVIEW should it go into more detail of possible compuations?
This document provides, first, an overview of the types and interfaces of JuliaPetra,
and second, comparisons with two similar libraries, Epetra and DistributedArrays.jl.

\section{Design of JuliaPetra}

The design of JuliaPetra follows closely with that of the C++ implementations of the
Petra object model, Epetra and Tpetra.
Like Epetra and Tpetra, JuliaPetra uses MPI and Single-Program-Multiple-Data as its base parallel model.
The API is split into two main layers of abstraction, the communication layer and the data structures layer.

The first level of abstraction handles the communication details and how the problem is
distributed across processes.
The \juliaSnippet{Comm} and \juliaSnippet{Distributor} types provide a low level interface to support
different communication methods.
All interprocess communication is done through \juliaSnippet{Comm} and \juliaSnippet{Distributor} objects.
So, new communication systems can be implemented without affected the objects built on top of it.
The \juliaSnippet{BlockMap}, \juliaSnippet{Directory}, \juliaSnippet{Import} and \juliaSnippet{Export}
types handle the distribution of the problem across the processors.
\juliaSnippet{BlockMap} and \juliaSnippet{Directory} manage which processor the various parts of
the problem are located.
\juliaSnippet{Import} and \juliaSnippet{Export} contain the logic behind redistributing
the problem among the processes.
\juliaSnippet{SrcDistObject} and \juliaSnippet{DistObject} provide an interface between
the redistributing logic and the data structures themselves.
The communication layer provides an abstraction on which the data structures can build.

The linear algebra objects are built on top of the abstractions provided by the communication layer.
JuliaPetra has two main linear algebra types.
The first type is the concrete type \juliaSnippet{MultiVector} which holds one or more dense vectors.
The second type is the \juliaSnippet{Operator} interface which is implemented by types that provide an
\(y = \alpha A x + \beta y\) operation, where \(A\) is the operator.
Matrices are one possible \juliaSnippet{Operator}, as implemented in the abstract type \juliaSnippet{RowMatrix},
which stores sparse matrices accessed by rows.
\juliaSnippet{RowGraph} is an additionally type used to represent the sparsity pattern of a \juliaSnippet{RowMatrix}.
Both \juliaSnippet{RowGraph} and \juliaSnippet{RowMatrix} have concrete implementations based on
compressed sparse row format, \juliaSnippet{CSRGraph} and \juliaSnippet{CSRMatrix} respectively.
Julia's \juliaSnippet{AbstractArray} is subtyped by both the \juliaSnippet{MultiVector}
and \juliaSnippet{RowMatrix} types to allow existing Julia code to interact with data in JuliaPetra objects.

\begin{figure}
	%name, type, location
	\newcommand{\typeNode} [3]{\node[#2] (#1) [#3] {\juliaSnippet{#1}};}
	\newcommand{\explicitInheritance}[2]{\draw[->] (#1.south) -- (#2);}
	\newcommand{\implicitInheritance}[2]{\draw[dashed,->] (#1.south) -- (#2);}
	\begin{tikzpicture}[
	ImplicitInterface/.style={rectangle, draw=black, dashed, minimum size=5mm},
	ExplicitInterface/.style={rectangle, draw=black, minimum size=5mm},
	ConcreteType/.style={rectangle, draw=black, very thick, minimum size=5mm}
	]
	
	\typeNode{Comm}{ExplicitInterface}{}
	
	\typeNode{MPIComm}{ConcreteType}{below=.75cm of Comm}
	\explicitInheritance{Comm}{MPIComm.north}
	
	\typeNode{SerialComm}{ConcreteType}{right=.25cm of MPIComm}
	\explicitInheritance{Comm}{SerialComm.north}
	
	\typeNode{LocalComm}{ConcreteType}{left=.25cm of MPIComm}
	\explicitInheritance{Comm}{LocalComm.north}
	
	\typeNode{Distributor}{ExplicitInterface}{left=5.1cm of Comm}
	
	\typeNode{SerialDistributor}{ConcreteType}{below right=.75cm and -1cm of Distributor}
	\explicitInheritance{Distributor}{SerialDistributor.north}
	
	\typeNode{MPIDistributor}{ConcreteType}{left=.25cm of SerialDistributor}
	\explicitInheritance{Distributor}{MPIDistributor.north}
	
	
	
	\typeNode{Directory}{ExplicitInterface}{below right=.75cm and -1.25cm of MPIDistributor}
	
	\typeNode{BasicDirectory}{ConcreteType}{below=.75cm of Directory}
	\explicitInheritance{Directory}{BasicDirectory.north}
	
	\typeNode{BlockMap}{ConcreteType}{right=of Directory}
	
	\typeNode{Import}{ConcreteType}{right=of BlockMap}
	
	\typeNode{Export}{ConcreteType}{right=of Import}
	
	
	
	\typeNode{SrcDistObject}{ImplicitInterface}{below left=.75cm and -.9cm of Export}
	
	\typeNode{DistObject}{ImplicitInterface}{below=.75cm of SrcDistObject}
	\implicitInheritance{SrcDistObject}{DistObject.north}
	
	\typeNode{Operator}{ImplicitInterface}{left=of DistObject}
	
	\typeNode{AbstractArray}{ExplicitInterface}{left=of Operator}
	
	\typeNode{MultiVector}{ConcreteType}{below=of AbstractArray}
	\implicitInheritance{DistObject}{MultiVector.north east}
	\explicitInheritance{AbstractArray}{MultiVector.north}
	
	\typeNode{RowMatrix}{ExplicitInterface}{below=of Operator}
	\implicitInheritance{DistObject}{RowMatrix.north east}
	\implicitInheritance{Operator}{RowMatrix.north}
	\explicitInheritance{AbstractArray}{RowMatrix.north west}
	
	\typeNode{RowGraph}{ExplicitInterface}{below=of DistObject}
	\implicitInheritance{DistObject}{RowGraph.north}
	
	\typeNode{CSRMatrix}{ConcreteType}{below=.75cm of RowMatrix}
	\explicitInheritance{RowMatrix}{CSRMatrix.north}
	
	\typeNode{CSRGraph}{ConcreteType}{below=.75cm of RowGraph}
	\explicitInheritance{RowGraph}{CSRGraph.north}
	
	
	
	\node[ExplicitInterface] (ExplicitKey) [below=of CSRMatrix] {Abstract Type};
	\node[ImplicitInterface] (ImplicitKey) [left=of ExplicitKey] {Implicit Type};
	\node[ConcreteType]      (ConcreteKey) [right=of ExplicitKey] {Concrete Type};
	
	\end{tikzpicture}
	\caption{Type hierarchy.}
	\label{fig:type-hierarchy}
\end{figure}


The type hierarchy in JuliaPetra is limited by the fact that types in Julia are restricted to a single supertype.
Interacting with existing code as a 2-dimensional array requires being a subtype of \juliaSnippet{AbstractArray}.
So, the other interfaces for the data structures are not explicit types,
but simply contracts promised in the documentation.
These implicit interfaces include \juliaSnippet{Operator}, \juliaSnippet{SrcDistObject}
and \juliaSnippet{DistObject}.
So, any methods that use one of those types accepts an object of any type and specifies that the necessary
methods are implemented in the documentation.
Figure~\ref{fig:type-hierarchy} shows the full type hierarchy.

\section{Comparisons with Other Distributed Libraries}

\subsection{Comparing with Epetra}

Epetra is the base implementation of the Petra object model,
written in a stable subset of C++. \cite{OverviewOfTrilinos}
Because Epetra was used as a template for implementing JuliaPetra,
the APIs for JuliaPetra are similar to those for Epetra.
The similarities between JuliaPetra and Epetra can be seen in how similar the respective implementations
of the power method are.
The differences in features between the implementation languages result in differences
between JuliaPetra's and Epetra's APIs.
For example, JuliaPetra takes advantage of higher level structures
such as type templating and thrown exceptions.

JuliaPetra is able to achieve the same performance as Epetra on large instances of the power method,
as shown in Table~\ref{tab:timing-results}.
Since Julia uses just in time compiling, the JuliaPetra power method has extra start up costs compared to
the Epetra version. However, since each specialized method needs to be compiled only once,
this is a fixed cost during the first evaluation.
Additionally, JuliaPetra uses runtime dispatch in a few locations, such as with
\juliaSnippet{Comm} objects, which adds additional overhead compared to Epetra.

\subsection{Comparing with DistributedArrays.jl}

DistributedArrays.jl is a similar library to JuliaPetra that is designed to support
distributed arrays in parallel environments. \cite{DAGithub}
DistributedArrays.jl is designed for Julia's built in model for parallelism, where the program logic is
contained in a master process that allocates work to the worker processes for expensive operations.
\cite{JuliaFreshApproach}
DistributedArrays.jl offers a number of advantages over JuliaPetra, however, it is not able to
match the speed of JuliaPetra.

DistributedArrays.jl has advantages over JuliaPetra because it is better designed to function with
existing Julia code.
For example, it is built around the default Julia parallelization APIs and model as well as using
Julia \juliaSnippet{AbstractArray}s for local storage.
Another advantage is that DistributedArrays.jl uses an instance of \juliaSnippet{AbstractArray}
for a process's local storage, this allows automatic support for different array structures.
Finally, by keeping the program logic on the master process, DistributedArrays.jl can be used to
do distributed computations while using Julia's read-eval-print loop.

However, DistributedArrays.jl is not able to match the speed of JuliaPetra.
JuliaPetra is able to run within half of the execution time of DistributedArrays.jl.
The difference in time is caused by JuliaPetra needing less inter-process communication.
%TODO is there other sources of improvement?

\section{Timing Results}

\begin{table}
\begin{tabular}{|c c|r|r|r||r|r|}
	\hline
		\multicolumn{2}{|c|}{Equations Per Process}
		& JuliaPetra
		& Epetra
		& \multicolumn{1}{m{1.8cm}||}{Distributed\-Arrays.jl}
		& \multicolumn{1}{m{1.75cm}|}{JuliaPetra / Epetra}
		& \multicolumn{1}{m{1.8cm}|}{JuliaPetra / Distributed\-Arrays.jl} \\
	\hline
		\multicolumn{7}{|l|}{4 Processors}\\
	\hline
		100,000			&Average & 0.27690 & 0.19294 & 1.86626 & 1.43515 & 0.14837 \\
		equations		&Minimum & 0.26948 & 0.18933 & 1.59244 & 1.42335 & 0.16922 \\
						&Maximum & 0.28782 & 0.19614 & 2.15375 & 1.46745 & 0.13364 \\
	\hline
		1,000,000		&Average & 3.07809 & 2.38844 & 14.7880 & 1.28875 & 0.20815 \\
		equations		&Minimum & 2.98883 & 2.35483 & 13.2876 & 1.26923 & 0.22493 \\
						&Maximum & 3.15854 & 2.44491 & 18.4698 & 1.29188 & 0.17101 \\
	\hline
		10,000,000		&Average & 33.0215 & 24.6116 & 108.760 & 1.34171 & 0.30362 \\
		equations		&Minimum & 31.5646 & 24.4134 & 107.118 & 1.29292 & 0.29467 \\
						&Maximum & 35.1871 & 24.7888 & 109.561 & 1.41947 & 0.32116 \\
	\hline
		\multicolumn{7}{|l|}{16 Processors}\\
	\hline
		100,000			&Average & 0.58371 & 0.51138 & 3.23072 & 1.15766 & 0.18068 \\
		equations		&Minimum & 0.54744 & 0.49328 & 2.99947 & 1.10979 & 0.18251 \\
						&Maximum & 0.60776 & 0.51907 & 3.47056 & 1.17086 & 0.17512 \\
	\hline
		1,000,000		&Average & 7.46367 & 7.39699 & 21.5343 & 1.00901 & 0.34659 \\
		equations		&Minimum & 7.36098 & 7.32456 & 20.0884 & 1.00497 & 0.36643 \\
						&Maximum & 7.59598 & 7.48964 & 22.8655 & 1.01420 & 0.33220 \\
	\hline
		10,000,000		&Average & 74.6001 & 73.6360 & 151.184 & 1.01310 & 0.49344 \\
		equations		&Minimum & 73.7350 & 73.4646 & 150.411 & 1.00368 & 0.49022 \\
						&Maximum & 74.9087 & 73.8342 & 152.033 & 1.01455 & 0.49271 \\
	\hline
			\multicolumn{7}{|l|}{20 Processors}\\
	\hline
		100,000			&Average & 0.73843 & 0.70837 & 3.14911 & 1.04244 & 0.23449 \\
		equations		&Minimum & 0.73621 & 0.69428 & 2.90237 & 1.06039 & 0.25366 \\
						&Maximum & 0.73967 & 0.71449 & 3.64133 & 1.03524 & 0.20313 \\
	\hline
		1,000,000		&Average & 9.19411 & 9.18584 & 23.2980 & 1.00090 & 0.39463 \\
		equations		&Minimum & 9.14960 & 9.16630 & 22.7740 & 0.99814 & 0.40177 \\
						&Maximum & 9.22002 & 9.21510 & 24.6296 & 1.00053 & 0.37435 \\
	\hline
		10,000,000		&Average & 92.0613 & 91.6512 & 174.402 & 1.00447 & 0.52786 \\
		equations		&Minimum & 91.5029 & 91.2491 & 173.480 & 1.00278 & 0.52746 \\
						&Maximum & 92.5630 & 91.7905 & 175.096 & 1.00842 & 0.52864 \\
	\hline
\end{tabular}

\caption{Timing results of various power method implementations.  All times are in seconds.}
\label{tab:timing-results}
\end{table}

As can be seen in Table~\ref{tab:timing-results}, JuliaPetra is very close to the performance
of Epetra for large problem sizes.
Additionally, JuliaPetra is able to significantly outperform DistributedArrays.jl, performing in less than
half the time of DistributedArrays.jl for all but the largest problems.

The timings come from implementations of the power method for finding the largest eigenvalue of a matrix.
\cite{PowerMethod}
This tests the performance of dot product, L2-norm and sparse matrix-vector multiplication functionalities.
The matrices used were various sizes of symmetric tridiagonal matrices with the diagonal containing 2's
and the off diagonals containing -1's distributed over different numbers of processors.
%TODO is there a name for this matrix?
Each combination of implementation, problem size and number of processors was run five times
and the average, minimum and maximum are presented.
The Julia implementations were run once before collecting the timings, to ensure all code was
compiled before the timing started.
The Julia tests were run with Julia v0.6.0 precompiled binary for Linux
with the \juliaSnippet{-O3} flag enabled at run time.
The Epetra tests were compiled with GCC 4.8.5, the \juliaSnippet{-O3} flag and Trilinos 12.10.1.
Tests were run on a Red Hat Enterprise Linux Workstation, version 7.3,
with 20 Intel Xeon CPUs.
%TODO anything else that should be described?


\section{Conclusion}

JuliaPetra is an implementation of the Petra object model in the Julia programming language.
So, it provides an interface for distributed solver algorithms to interact and build on each other.
Additionally, by performing at the same speeds as Epetra,
it allows Julia to compete with C++ for these types of high performance computations.

\bibliography{bibliography}
\end{document}