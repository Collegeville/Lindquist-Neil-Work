\documentclass[acmsmall]{acmart}
\bibliographystyle{ACM-Reference-Format}

\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{positioning} 

\title{Implementation of Petra in Julia}
\author{Neil Lindquist}

\newcommand{\juliaSnippet}[1]{\texttt{\detokenize{#1}}}

\let\oldtextregister\textregistered
\renewcommand{\textregistered}{\textsuperscript{\tiny\oldtextregister}}

\begin{document}

\maketitle

\section{Introduction}

%REVIEW include description of Julia?
Julia is a programing language first publicly released in 2012 \cite{WhyWeCreatedJulia}.
Julia is designed so that it
``has the performance of a statically compiled
language while providing interactive dynamic behavior and productivity like Python, LISP or
Ruby'' \cite{JuliaDesignPaper}.

%TODO go into more detail?
The Petra Object Model is the design used in Trilinos for objects commonly used in solver algorithms.
\cite{OverviewOfTrilinos}
Petra libraries provide parallel, distributed matrices, vectors and graphs for other packages to use.
The Petra object model has previously been implemented in a very stable subset of C++,
in a newer, more advanced subset of C++ and in Java.
%REVIEW should this list of previous implemenetations be a thing

JuliaPetra is an implementation of the Petra object model in Julia.
The design of JuliaPetra follows closely with that of the C++ implementations of the
Petra object model, Epetra and Tpetra.
In particular, the communication APIs, such as \juliaSnippet{Comm} and \juliaSnippet{BlockMap}
are based directly off Epetra and the data structures, such as \juliaSnippet{MultiVector} and
\juliaSnippet{RowMatrix} are based directly off Tpetra.
Additionally, like Epetra and Tpetra, JuliaPetra uses MPI and Single-Program-Multiple-Data as it's parallel model.

\begin{figure}[h]
	%name, type, location
	\newcommand{\typeNode} [3]{\node[#2] (#1) [#3] {\juliaSnippet{#1}};}
	\newcommand{\explicitInheritance}[2]{\draw[->] (#1.south) -- (#2.north);}
	\newcommand{\implicitInheritance}[2]{\draw[dashed,->] (#1.south) -- (#2.north);}
	\begin{tikzpicture}[
		ImplicitInterface/.style={rectangle, draw=black, dashed, minimum size=5mm},
		ExplicitInterface/.style={rectangle, draw=black, minimum size=5mm},
		ConcreteType/.style={rectangle, draw=black, very thick, minimum size=5mm}
		]
		
		\typeNode{SrcDistObject}{ImplicitInterface}{}
		
		\typeNode{DistObject}{ImplicitInterface}{below=.75cm of SrcDistObject}
		\implicitInheritance{SrcDistObject}{DistObject}
		
		\typeNode{Operator}{ImplicitInterface}{left=of DistObject}
		
		\typeNode{AbstractArray}{ExplicitInterface}{left=of Operator}
		
		\typeNode{MultiVector}{ConcreteType}{below=of AbstractArray}
		\implicitInheritance{DistObject}{MultiVector}
		\explicitInheritance{AbstractArray}{MultiVector}
		
		\typeNode{RowMatrix}{ExplicitInterface}{below=of Operator}
		\implicitInheritance{DistObject}{RowMatrix}
		\implicitInheritance{Operator}{RowMatrix}
		\explicitInheritance{AbstractArray}{RowMatrix}
		
		\typeNode{RowGraph}{ExplicitInterface}{below=of DistObject}
		\implicitInheritance{DistObject}{RowGraph}
		
		\typeNode{CSRMatrix}{ConcreteType}{below=.75cm of RowMatrix}
		\explicitInheritance{RowMatrix}{CSRMatrix}
		
		\typeNode{CSRGraph}{ConcreteType}{below=.75cm of RowGraph}
		\explicitInheritance{RowGraph}{CSRGraph}
		
		
		\typeNode{Import}{ConcreteType}{left=of SrcDistObject}
		
		\typeNode{Export}{ConcreteType}{left=of Import}
		
		
		\typeNode{Comm}{ExplicitInterface}{left=of CSRMatrix}
		
		\typeNode{SerialComm}{ConcreteType}{below right=of Comm}
		\explicitInheritance{Comm}{SerialComm}
		
		\typeNode{MPIComm}{ConcreteType}{left=.25cm of SerialComm}
		\explicitInheritance{Comm}{MPIComm}
		
		\typeNode{LocalComm}{ConcreteType}{right=.25cm of SerialComm}
		\explicitInheritance{Comm}{LocalComm}
		
		
		\typeNode{Directory}{ExplicitInterface}{left=of AbstractArray}
		
		\typeNode{BasicDirectory}{ConcreteType}{below=of Directory}
		\explicitInheritance{Directory}{BasicDirectory}
		
		\typeNode{BlockMap}{ConcreteType}{left=of Export}
		
		
		\typeNode{Distributor}{ExplicitInterface}{left=.75cm of Comm}
		
		\typeNode{SerialDistributor}{ConcreteType}{below=of Distributor}
		\explicitInheritance{Distributor}{SerialDistributor}
		
		\typeNode{MPIDistributor}{ConcreteType}{left=.25cm of SerialDistributor}
		\explicitInheritance{Distributor}{MPIDistributor}
		
		\node[ImplicitInterface] (ImplicitKey) [below=of MPIDistributor] {Implicit Type};
		\node[ExplicitInterface] (ExplicitKey) [right=of ImplicitKey] {Explicit, Abstract Type};
		\node[ConcreteType]      (ConcreteKey) [right=of ExplicitKey] {Concrete Type};

	\end{tikzpicture}
	\caption{Type hierarchy.}
	\label{fig:type-hierarchy}
\end{figure}

JuliaPetra's type hierarchy is built on top of Julia's \juliaSnippet{AbstractArray} type.
This improves interaction between JuliaPetra and existing Julia code.
Since Julia doesn't support multiple inheritance, %TODO cite?
other interfaces are implemented without an explicit type.
These interfaces include \juliaSnippet{Operator}, \juliaSnippet{SrcDistObject} and \juliaSnippet{DistObject}.
So, any methods that use one of those types accepts any object and assumes the methods required for the interface are present.
Figure~\ref{fig:type-hierarchy} contains the full type hierarchy.

Since JuliaPetra follows the design of Epetra and Tpetra, the library provides the ability to do similar
computations as the existing Petra implementations.
For example, JuliaPetra supports \juliaSnippet{MultiVector} operations such as dot product and norms.
Additionally, sparse matrix - vector products can be computed.

\section{Comparisons with Other Distributed Libraries}

\subsection{Comparing with Epetra}

The APIs for JuliaPetra are fairly similar to those for Epetra,
as Epetra was used as the template for implementing JuliaPetra.
There are some differences between the two.
For example, JuliaPetra takes advantage of higher level structures, such as thrown exceptions and
the \juliaSnippet{AbstractArray} support, where able.
The similarities between JuliaPetra and Epetra can be seen in how similar the respective implementations
of the power method are.

JuliaPetra was able to achieve the same performance as Epetra on large instances of the power method,
as can be seen in the Results section.
Since Julia uses just in time compiling exclusively, it has extra start up costs compared to
the Epetra version. However, since each specialized method needs to be compiled only once,
this is a fixed cost.
Additionally, JuliaPetra uses runtime dispatch in a few locations, especially related to
\juliaSnippet{Comm} objects.  This will also add overhead compared to the Epetra version.

\subsection{Comparing with DistributedArrays.jl}

DistributedArrays.jl is a similar library designed to support distributed arrays in parallel environments.
\cite{DAGithub}
%TODO talk about general design
DistributedArrays.jl is designed for Julia's built in parallel model, so operations that apply to
each element are distributed to each process from the master process.
DistributedArrays.jl offers a number of advantages over JuliaPetra, however, it is not able to
match the speed of JuliaPetra.

One area where DistributedArrays.jl has an advantage over Julia is that it follows standard
practice for ideomatic Julia code more than JuliaPetra.
For example, it is built around the default Julia parallelization model and APIs.
It's APIs are all designed as function calls that should be made from a main process that
then delegates part of the work to the other processes. %TODO remove?
%TODO comment how DA fits a little better with Array APIS
	%does it majorly?
Another advantage is that DistributedArrays.jl uses an instance of \juliaSnippet{AbstractArray}
for a process's local storage, this allows automatic support for different array structures.

However, DistributedArrays.jl was not able to match the speed of JuliaPetra.
It was able to get within twice the time of JuliaPetra, but was never able to get within 1.5 times
the execution time of JuliaPetra.

\section{Results}

%TODO figure out the best way to lay this out
\begin{figure}[h]
\begin{tabular}{|c c|r|r|r||r|r|}
	\hline
		\multicolumn{2}{|c|}{Equations Per Process}
		& JuliaPetra
		& Epetra
		& \multicolumn{1}{m{1.8cm}||}{Distributed\-Arrays.jl}
		& \multicolumn{1}{m{1.75cm}|}{JuliaPetra / Epetra}
		& \multicolumn{1}{m{1.8cm}|}{JuliaPetra / Distributed\-Arrays.jl} \\
	\hline
		\multicolumn{7}{|l|}{4 Processors}\\
	\hline
		100,000			&Average & 0.27690 & 0.19294 & 1.86626 & 1.43515 & 0.14837 \\
		equations		&Minimum & 0.26948 & 0.18933 & 1.59244 & 1.42335 & 0.16922 \\
						&Maximum & 0.28782 & 0.19614 & 2.15375 & 1.46745 & 0.13364 \\
	\hline
		1,000,000		&Average & 3.07809 & 2.38844 & 14.7880 & 1.28875 & 0.20815 \\
		equations		&Minimum & 2.98883 & 2.35483 & 13.2876 & 1.26923 & 0.22493 \\
						&Maximum & 3.15854 & 2.44491 & 18.4698 & 1.29188 & 0.17101 \\
	\hline
		10,000,000		&Average & 33.0215 & 24.6116 & 108.760 & 1.34171 & 0.30362 \\
		equations		&Minimum & 31.5646 & 24.4134 & 107.118 & 1.29292 & 0.29467 \\
						&Maximum & 35.1871 & 24.7888 & 109.561 & 1.41947 & 0.32116 \\
	\hline
		\multicolumn{7}{|l|}{16 Processors}\\
	\hline
		100,000			&Average & 0.58371 & 0.51138 & 3.23072 & 1.15766 & 0.18068 \\
		equations		&Minimum & 0.54744 & 0.49328 & 2.99947 & 1.10979 & 0.18251 \\
						&Maximum & 0.60776 & 0.51907 & 3.47056 & 1.17086 & 0.17512 \\
	\hline
		1,000,000		&Average & 7.46367 & 7.39699 & 21.5343 & 1.00901 & 0.34659 \\
		equations		&Minimum & 7.36098 & 7.32456 & 20.0884 & 1.00497 & 0.36643 \\
						&Maximum & 7.59598 & 7.48964 & 22.8655 & 1.01420 & 0.33220 \\
	\hline
		10,000,000		&Average & 74.6001 & 73.6360 & 151.184 & 1.01310 & 0.49344 \\
		equations		&Minimum & 73.7350 & 73.4646 & 150.411 & 1.00368 & 0.49022 \\
						&Maximum & 74.9087 & 73.8342 & 152.033 & 1.01455 & 0.49271 \\
	\hline
			\multicolumn{7}{|l|}{20 Processors}\\
	\hline
		100,000			&Average & 0.73843 & 0.70837 & 3.14911 & 1.04244 & 0.23449 \\
		equations		&Minimum & 0.73621 & 0.69428 & 2.90237 & 1.06039 & 0.25366 \\
						&Maximum & 0.73967 & 0.71449 & 3.64133 & 1.03524 & 0.20313 \\
	\hline
		1,000,000		&Average & 9.19411 & 9.18584 & 23.2980 & 1.00090 & 0.39463 \\
		equations		&Minimum & 9.14960 & 9.16630 & 22.7740 & 0.99814 & 0.40177 \\
						&Maximum & 9.22002 & 9.21510 & 24.6296 & 1.00053 & 0.37435 \\
	\hline
		10,000,000		&Average & 92.0613 & 91.6512 & 174.402 & 1.00447 & 0.52786 \\
		equations		&Minimum & 91.5029 & 91.2491 & 173.480 & 1.00278 & 0.52746 \\
						&Maximum & 92.5630 & 91.7905 & 175.096 & 1.00842 & 0.52864 \\
	\hline
\end{tabular}

\caption{Timing results in seconds.}
\label{fig:timing-results}
\end{figure}

The timing results can be seen in Figure~\ref{fig:timing-results}.
The timing results were obtained by running the various power method implementations
with various sized problems and varying numbers of processors.
Each combination of problem size and number of processors was run five times.
Each Julia implementation was run before making any timings to ensure all code was
compiled before the timing started.
Tests were run with Julia v0.6.0 and Trilinos 12.10.1.
Tests were run on a Red Hat Enterprise Linux Workstation, version 7.3,
with 20 Intel\textregistered Xeon\textregistered cpu's.
%TODO anything else that should be described?


\section{Conclusion}

JuliaPetra is an implementation of the Petra object model in the Julia programming language.
This provides a set of basic, distributed linear algebra objects for building solvers in Julia.
JuliaPetra is able to perform at the same speeds as Epetra and so, allows implementation of
solver code in Julia with the distributed linear algebra support of the Petra object model.

%This shows that...?

\bibliography{bibliography}
\end{document}